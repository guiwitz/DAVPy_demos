{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3ffaa4-d8ce-4e2d-826c-8301c8bc9075",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In order to run the following models on the cluster, you need to follow these steps:\n",
    "\n",
    "1. Create an environment called nlp. It contains ipykernel (to make the kernel available) and a utility to show progress bars:\n",
    "   \n",
    "    `conda create -n nlp python=3.12 ipykernel ipywidgets`\n",
    "\n",
    "2. Activate the environment\n",
    "\n",
    "   `conda activate nlp`\n",
    "\n",
    "3. Load a specific version of CUDA\n",
    "\n",
    "    `module load CUDA/12.6.0`\n",
    "\n",
    "4. Install PyTorch. You have to make sure the it is compatible with your CUDA version. Check that on https://pytorch.org/get-started/locally/. For the above selected version we can just run:\n",
    "\n",
    "`pip install torch torchvision torchaudio`\n",
    "\n",
    "5. Install additional packages to handle deep learning models:\n",
    "\n",
    "`pip install transformers accelerate qwen-vl-utils`\n",
    "\n",
    "7. Make the kernel available:\n",
    "\n",
    "    `python -m ipykernel install --user --name nlp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a5889-bf7f-4d33-b45f-8a5eb4a1c870",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Analyse if a text is positive or negative with https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248c2f12-47b0-482f-8a1d-443f4cd61763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9849875569343567}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_task = pipeline(\"sentiment-analysis\",\n",
    "                          model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                          tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "sentiment_task(\"This is a great movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfe627d-1660-47b2-a7e3-b4bf8303650a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.5444068312644958}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task(\"One one side the actors were great, on the other the photography is bad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6517ad-02b8-4988-b549-51354ef5d905",
   "metadata": {},
   "source": [
    "## Describing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74a668-cef4-4544-a30d-e56ed5d7ebe0",
   "metadata": {},
   "source": [
    "Describe an image using a multimodal model: https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331d5305-6a7a-4fa4-a7ea-d53e0df45a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7fea74-cf1f-4ea8-89d8-4f00ad976418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\\n    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\\n)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c272b0-5b37-4e3c-9839-a3689e884cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb980af05534f71b70b44bf5bc49b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55da773b-3c82-4007-bb0d-540405f56a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image depicts two ducks in a shallow, rocky body of water. The duck in the foreground is a male mallard, characterized by its green head and white and brown body. The duck in the background is a female mallard, with a similar coloration but with a more muted brown and white plumage. The water is clear, allowing the rocks beneath to be visible, and the ducks are standing on the rocky shore. The scene is serene and natural, with the ducks appearing to be at ease in their environment.']\n"
     ]
    }
   ],
   "source": [
    "#processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"ducks.jpg\" ,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c2469-9894-4a61-b80c-05c28281467d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
